import json
import os
import argparse
from datetime import datetime
import random

models = ['llama_7B', 'llama_13B', 'llama_30B', 'llama_65B', 'mistral_7B', 'falcon_7B']

def pair_data(input_dir):
    # Ensure input directory exists
    if not os.path.exists(input_dir):
        raise FileNotFoundError("Directory doesn't exist")
    
    # Filename for the NYT articles extracted human data
    fn = "nyt_parsed.json"
    
    # Store the ids for the articles used in the machine translated files
    ids = set()
    # For each model, store each article _id as a key with value equal to its lead_paragraph generated by LLM
    temp = [{}]
    res = temp[0]
    for i in range(len(models)):
        model = models[i]
        model_fn = f"{model}.json"
        with open(os.path.join(input_dir, model_fn), "r") as file:
            res[model] = {}
            data = json.load(file)
            # print(f"{model_fn} has {len(data)} entries")
            for entry in data:
                _id = entry["_id"]
                lead_paragraph = entry["lead_paragraph"]
                ids.add(_id)
                res[model][_id] = lead_paragraph
    
    # Now pair the extracted data from the LLM generated files and pair with the human generated file
    paired = [[] for i in range(len(models))]
    with open(os.path.join(input_dir, fn), "r") as file:
        data = json.load(file)
        for entry in data:
            _id = entry["_id"]
            if _id not in ids:
                continue
            for i in range(len(models)):
                model = models[i]
                paired[i].append({"human": entry["lead_paragraph"], "llm": res[model][_id]})
          
    # Write paired data to separate files (both JSON and JSONL formats)
    for i in range(len(models)):
        model = models[i]
        
        # Write JSON format
        model_fn = f"{model}_paired.json"
        with open(os.path.join(input_dir, model_fn), "w") as outfile:
            # print(f"{model_fn}_paired has {len(paired[i])} entries")
            json.dump(paired[i], outfile)
        
        # Write JSONL format
        model_fn_jsonl = f"{model}_paired.jsonl"
        with open(os.path.join(input_dir, model_fn_jsonl), "w") as outfile:
            for entry in paired[i]:
                json.dump(entry, outfile)
                outfile.write('\n')
   
def split_data(input_dir, train_split, seed=42):
    
    random.seed(seed)
    
    for model in models:
        model_fn = f"{model}_paired.json"
        with open(os.path.join(input_dir, model_fn), "r") as file:
            data = json.load(file)
    
        shuffled_list = list(data)  # Create a copy to avoid modifying the original list
        random.shuffle(shuffled_list)

        split_index = int(len(shuffled_list) * 0.8)
        
        train_data = shuffled_list[:split_index]
        test_data = shuffled_list[split_index:]
        
        # Write train data (JSON and JSONL)
        train_fn = f"{model}_train.json"
        with open(os.path.join(input_dir, train_fn), "w") as outfile:
            json.dump(train_data, outfile)
        
        train_fn_jsonl = f"{model}_train.jsonl"
        with open(os.path.join(input_dir, train_fn_jsonl), "w") as outfile:
            for entry in train_data:
                json.dump(entry, outfile)
                outfile.write('\n')
        
        # Write test data (JSON and JSONL)
        test_fn = f"{model}_test.json"
        with open(os.path.join(input_dir, test_fn), "w") as outfile:
            json.dump(test_data, outfile)
        
        test_fn_jsonl = f"{model}_test.jsonl"
        with open(os.path.join(input_dir, test_fn_jsonl), "w") as outfile:
            for entry in test_data:
                json.dump(entry, outfile)
                outfile.write('\n')
        
        # print(f"{model_fn} train_data len: {len(train_data)}")
        # print(f"{model_fn} test_data len: {len(test_data)}")

def main():
    parser = argparse.ArgumentParser(description='Pair NYT Archive human data with LLM generated data')
    # parser.add_argument('filename', type=str, help='filename of the human generated NYT data')
    parser.add_argument('input_dir', type=str, help='Input directory of the downloaded data')

    args = parser.parse_args()
    
    pair_data(args.input_dir)
    split_data(args.input_dir, .8)

if __name__ == '__main__':
    main()